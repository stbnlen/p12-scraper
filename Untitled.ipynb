{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "303d7ec1-f635-419d-9581-26e03666e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "from functions import Pagina12Scraper\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e6f2e4-5717-4c87-95fc-234d7d75af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pagina12Scraper:\n",
    "    def __init__(self, base_url: str):\n",
    "        \"\"\"\n",
    "        Constructor for Pagina12Scraper class.\n",
    "\n",
    "        Args:\n",
    "            base_url (str): Base URL of the webpage to scrape.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def get_link_sections(self) -> list:\n",
    "        \"\"\"\n",
    "        Get the links of the news sections from the webpage.\n",
    "\n",
    "        Returns:\n",
    "            list: List of links of the news sections.\n",
    "        \"\"\"\n",
    "        # Get the content of the webpage\n",
    "        response = requests.get(self.base_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"lxml\")\n",
    "        \n",
    "        # Get news sections\n",
    "        sections = soup.find('div', attrs={\"class\": \"p12-dropdown-column\"}).find_all(\"a\")\n",
    "\n",
    "        # Get links of news sections and exclude specific elements\n",
    "        link_sections = [section.get(\"href\") for section in sections if section.get(\"href\") not in ['https://www.pagina12.com.ar/suplementos/rosario12', 'https://www.pagina12.com.ar/suplementos/cultura-y-espectaculos', 'https://www.pagina12.com.ar//buenos-aires12', 'https://www.pagina12.com.ar/edicion-impresa', 'https://www.pagina12.com.ar//suplementos/soy', 'https://www.pagina12.com.ar//suplementos/las12']]\n",
    "        \n",
    "        return link_sections\n",
    "        \n",
    "    def get_links(self, soup: BeautifulSoup) -> list:\n",
    "        \"\"\"\n",
    "        Get the links of the news from a web page.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object of the web page.\n",
    "\n",
    "        Returns:\n",
    "            list: List of links of the news.\n",
    "        \"\"\"\n",
    "        links = []\n",
    "        sections = soup.find('div', attrs={\"class\": \"main-content\"})\n",
    "\n",
    "        # Get link of the featured news\n",
    "        top_content = sections.find(\"section\", attrs={\"class\":\"top-content\"})\n",
    "        if top_content:\n",
    "            top_content_link = top_content.a.get(\"href\")\n",
    "            if top_content_link:\n",
    "                links.append(f\"{self.base_url}{top_content_link}\")\n",
    "\n",
    "        # Get links of the news from the content list\n",
    "        list_content = sections.find(\"section\", attrs={\"class\":\"list-content\"})\n",
    "        if list_content:\n",
    "            list_content_articles = list_content.find_all(\"article\")\n",
    "            for article in list_content_articles:\n",
    "                article_link = article.find(\"a\").get(\"href\")\n",
    "                if article_link:\n",
    "                    links.append(f\"{self.base_url}{article_link}\")\n",
    "\n",
    "        return links\n",
    "\n",
    "    def get_note(self, url_nota: str) -> dict:\n",
    "        \"\"\"\n",
    "        Get the details of a news article from its URL.\n",
    "\n",
    "        Args:\n",
    "            url_nota (str): URL of the news article.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary with the details of the news article, including title,\n",
    "                  date, lead, subtitle, tags, and body content of the news article.\n",
    "        \"\"\"\n",
    "        noticia = {}\n",
    "        try:\n",
    "            nota = requests.get(url_nota)\n",
    "            nota.raise_for_status() # Raise an exception if the request returns an error status code\n",
    "            s_nota = BeautifulSoup(nota.text, 'lxml').find(\"article\", attrs={\"class\":\"article-full section\"})\n",
    "            titulo = s_nota.find(\"h1\").text\n",
    "            fecha = s_nota.find(\"time\").get(\"datetime\")\n",
    "            copete = s_nota.find(\"h2\", attrs={\"class\":\"h3\"}).text\n",
    "            volanta = s_nota.find(\"h2\", attrs={\"class\":\"h4\"}).text\n",
    "            tags = s_nota.find_all(\"a\", attrs={\"class\": \"tag\"})\n",
    "            body_texts = s_nota.find(\"div\", attrs={\"class\": \"article-main-content article-text\"}).find_all(\"p\")\n",
    "\n",
    "            noticia = {\n",
    "                \"title\": titulo,\n",
    "                \"date\": fecha,\n",
    "                \"lead\": copete,\n",
    "                \"subtitle\": volanta,\n",
    "                \"tags\": [tag.text for tag in tags],\n",
    "                \"body\": [body_text.text for body_text in body_texts]\n",
    "            }\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error:\")\n",
    "            print(e)\n",
    "        return noticia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0265686c-8a0c-458b-927a-fee48f422aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the webpage to scrape\n",
    "URL = \"https://www.pagina12.com.ar\"\n",
    "\n",
    "# Create an instance of Pagina12Scraper\n",
    "scraper = Pagina12Scraper(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c97e854a-4e23-49c0-90d0-c5fac998d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/537401-el-mapa-desmesurado-de-una-viajera-mental\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/529247-pinturas-intensas-de-un-pensamiento-en-accion\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/539929-amor-y-odio-pasiones-del-ser\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/538227-el-yo-y-el-otro-en-las-redes-sociales\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/534098-la-herida-abierta-40-anos-despues\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/531945-novela-nac-pop-del-neurotico\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/494103-alicia-entel-politica-y-creencia-el-giro-fascista\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/481588-los-lapices-siguen-escribiendo-emilce-moler-conversa-con-sil\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/448110-mempo-giardinelli-conversa-con-karina-micheletto\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/446412-beneficios-exclusivos-para-soci-s-de-pagina-12\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/437984-alud-en-villa-huinid-accidente-natural-o-falla-humana-preven\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/535616-las-deudas-de-la-democracia\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/529486-solo-una-reina-podra-salvarnos\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/538909-conjeturas-sobre-el-narrar-los-diarios-y-la-caligrafia\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/537216-cuando-la-sonrisa-se-apaga\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/539912-guillermo-patricio-lobo-y-liver-trinidad-espinoza\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/539034-julio-antonio-altamirano-agustin-hilario-ulrich-companeros-d\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/538683-nidia-beatriz-munoz-y-luis-ramon-ledesma-martin-maria-pereir\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/536930-edith-vilma-deleon-y-desaparecidos-de-el-vesubio-y-puente-12\n",
      "Error: 429 Client Error: Too Many Requests for url: https://www.pagina12.com.ar/536347-pedro-antonio-cordero-suarez\n"
     ]
    }
   ],
   "source": [
    "link_sections = scraper.get_link_sections()\n",
    "\n",
    "# Lista para almacenar enlaces de noticias\n",
    "news_list = []\n",
    "\n",
    "# Obtener enlaces de noticias de cada sección de enlaces\n",
    "for link_section in link_sections:\n",
    "    try:\n",
    "        r = requests.get(link_section)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        # Obtener enlaces de noticias\n",
    "        news_links = scraper.get_links(soup)\n",
    "        # Agregar enlaces de noticias a la lista\n",
    "        news_list.extend(news_links)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f'Error al obtener enlaces de noticias de la sección {link_section}: {e}')\n",
    "    except Exception as e:\n",
    "        print(f'Error general al obtener enlaces de noticias de la sección {link_section}: {e}')\n",
    "\n",
    "# Obtener detalles de noticias en una sola llamada\n",
    "news_data = []\n",
    "for url in news_list:\n",
    "    try:\n",
    "        news_data.append(scraper.get_note(url))\n",
    "    except Exception as e:\n",
    "        print(f'Error al obtener detalles de noticias de la URL {url}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110e6676-6324-4920-bb9d-55c9c76b209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from list of dictionaries\n",
    "df = pd.DataFrame(news_data)\n",
    "\n",
    "# Drop rows with null values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Save DataFrame to a CSV file\n",
    "df.to_csv(\"news.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
